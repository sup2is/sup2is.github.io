---

layout: post
title: "Kafka Streams로 데이터 파이프라인 구축하기"
tags: [Apache Kafka, Kafka Streams]
date: 2020-06-09
comments: true
---



<br>

# OverView

이번시간에는 Kafka Streams에 대해서 알아보고 카프카 스트림을 사용해서 실시간으로 실시간으로 단어수를 체크하는 애플리케이션을 만들어보도록 하겠다!

# Kafka Streams란?

Kafka Streams는 효율적인 실시간 스트리밍 처리를 위해 만들어진 라이브러리이고 기본적으로 카프카에 포함되어있다.

이 카프카 스트림은 데이터를 실시간으로 처리하기때문에 데이터를 한꺼번에 다루는 배치모델과는 조금 대조되는 형태이다. 배치 처리같은 경우는 job이라는 단위로 실행되어 시작과 끝이 분명하지만 스트림처리는 시작과 끝이 불분명하기 때문에 스트림처리에는 항상 '실시간'이라는 단어가 따라온다.

카프카 스트림은 consumer 역할로 토픽에서 데이터를 꺼내온 뒤 Stream DSL을 사용해서 데이터를 가공 후 다시 또다른 토픽으로 데이터를 밀어 넣어줄 수 있다.

이 카프카 스트림을 직접 구현해보기위해 간단한 예제를 짜봤다.



# Kafka Streams를 통해 구축하는 단어 개수 카운팅 애플리케이션

일단 시나리오는 다음과 같다. 매우 간단하게 구성했다.

1. kafka-console-producer를 사용해서 텍스트 데이터를 실시간으로 **streams-plaintext-input**토픽에 넣는다.
2. streams-plaintext-input토픽의 컨슈머인 WordCount 모듈은 카프카 스트림 api를 사용해서 텍스트를 공백 단위로 구분, 현재까지 들어온 단어들을 카운팅해서 **streams-wordcount-output**토픽에 넣는다.
3. kafka-console-consumer를 사용해서 실시간으로 streams-wordcount-output 토픽의 내용을 확인한다.

전체적인 아키텍쳐는 다음과 같다.







# 시작하기 전에

1. 기본적으로 카프카 서버가 한대 이상 있다고 가정하고 예제를 진행하도록 하겠다. 나같은경우는 도커 컨테이너를 구성해서 클러스터를 구성했다. docker-compose 사용이 가능하다면  [https://github.com/simplesteph/kafka-stack-docker-compose](https://github.com/simplesteph/kafka-stack-docker-compose) 을 참고해서 아주 간단하게 클러스터를 구성할 수 있다. 
2. Kafka Streams를 사용하기 위해 관련 파일들을 [https://www.confluent.io/download에서](https://www.confluent.io/download) 받는다. (이 예제는 Confluent Platform 기반으로 작성한다.)
3. [https://github.com/confluentinc/kafka-streams-examples/tree/master](https://github.com/confluentinc/kafka-streams-examples/tree/master) kafka-streams에 관해서 예제 깃헙이 제공되고 있다. 관심있으신 분도 있을 것 같아서 추가했다.
4. 이 예제는 java로 작성되었다.



# 토픽 만들기

스트림 예제에서 사용할 토픽을 먼저 만들어준다. **streams-plaintext-input** 토픽은 공백으로 이루어진 데이터들을 밀어넣고 **streams-wordcount-output** 토픽은 스트림 처리된 데이터를 넣어줄 예정이다.

```

# Create the input topic
  ./bin/kafka-topics --create \
          --bootstrap-server localhost:9092,localhost:9093,localhost:9094 \
          --replication-factor 3 \
          --partitions 3 \
          --topic streams-plaintext-input

# Create the output topic
  ./bin/kafka-topics --create \
          --bootstrap-server localhost:9092,localhost:9093,localhost:9094 \
          --replication-factor 3 \
          --partitions 3 \
          --topic streams-wordcount-output
```





# WordCount 애플리케이션 구현하기











# 테스트해보기





```
./bin/kafka-console-consumer --bootstrap-server localhost:9092,localhost:9093,localhost:9094 \
        --topic streams-wordcount-output \
        --from-beginning \
        --formatter kafka.tools.DefaultMessageFormatter \
        --property print.key=true \
        --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
        --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer
```



# 마무리

실전 아파치 카프카에서는 Fluentd + influxDB + Grafana + Kafka Streams를 통해 카프카 브로커의 JMX 모니터링을 다루는 예제가 있었다. 뭔가 설정할것도 많고 카프카 스트림을 보자고 저 많은것들을 다 본다는게 약간 배보다 배꼽이 더 큰 느낌이여서 포스팅으로는 작성하지 않았다. 이것 외에 딱히 별다른 예제가 없어서 다른 baeldung이나 confluent 에서 제공하는 wordcounting 애플리케이션을 예제로 작성했다.









<hr>
포스팅은 여기까지 하겠습니다. 퍼가실때는 출처를 반드시 남겨주세요!


<br>

**References**

-  [https://docs.confluent.io/current/streams/quickstart.html](https://docs.confluent.io/current/streams/quickstart.html)
-  [https://www.baeldung.com/java-kafka-streams](https://www.baeldung.com/java-kafka-streams)
-  실전 아파치 카프카 - 사사키 도루 등 6명 (한빛미디어)
